{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finnish-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "needed-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ld_json(url: str) -> dict:\n",
    "    parser = \"html.parser\"\n",
    "    req = requests.get(url)\n",
    "    page = soup(req.text, parser)\n",
    "    return json.loads(\"\".join(page.find(\"script\", {\"type\":\"application/ld+json\"}).contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "brutal-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredients Function\n",
    "\n",
    "def get_ingredientScript(url):\n",
    "    jsonld = get_ld_json(url)\n",
    "    useful = jsonld[1]\n",
    "    ingredients = useful[\"recipeIngredient\"]\n",
    "    return ingredients\n",
    "\n",
    "def parse_ingredients(url):\n",
    "    ingredients = get_ingredientScript(url)\n",
    "    df = DataFrame (ingredients,columns=['ingredients'])\n",
    "    # 2 cases\n",
    "    # \"number\" (\"()\") (\"unit\") (adjective) \"noun/subject - ingredient\" (, other)\n",
    "    # contains \"to taste\"\n",
    "    df = df[\"ingredients\"]\n",
    "    df_taste = df[df.str.contains('to taste', case = False)]\n",
    "    \n",
    "    df_unit = df[~df.str.contains('to taste', case = False)]\n",
    "    \n",
    "    # array of arrays: each array is amount, unit, ingredient, descriptor, preparation\n",
    "    ingredients_parsed = []\n",
    "    \n",
    "    for i in df:\n",
    "        curr_arr = [\"\", \"\", \"\", \"\", \"\"]\n",
    "        print(i)\n",
    "        if 'to taste' not in i:\n",
    "        \n",
    "            # before we look at POS, remove everything after the comma and put it in prep\n",
    "            split_string = i.split(\", \", 1)\n",
    "            root_phrase = split_string[0]\n",
    "            if len(split_string) > 1:\n",
    "                other_piece = split_string[1]\n",
    "            else:\n",
    "                other_piece = \"\"\n",
    "\n",
    "            curr_arr[4] = other_piece\n",
    "\n",
    "            split_string2 = root_phrase.split(\"(\", 1)\n",
    "            if len(split_string2) > 1:\n",
    "                split_string3 = split_string2[1].split(\")\", 1)\n",
    "                curr_arr[3] = split_string3[0]\n",
    "                root_phrase = split_string2[0].strip() + split_string3[1]\n",
    "            doc = nlp(root_phrase)\n",
    "            index = 0\n",
    "            for token in doc:\n",
    "                found_num = False\n",
    "                # only get first number if it matches criteria\n",
    "                #if found_num == False and token.pos_ == \"NOUN\" and not token.is_alpha or token.pos_ == \"NUM\":\n",
    "                if index == 0:\n",
    "                    curr_arr[0] = token.text\n",
    "                elif index == 1:\n",
    "                    if token.pos_ != \"ADJ\":\n",
    "                        curr_arr[1] = token.text\n",
    "                    else:\n",
    "                        curr_arr[3] = token.text\n",
    "                elif token.dep_ == \"ROOT\":\n",
    "                    curr_arr[2] = token.text\n",
    "                else: \n",
    "                    curr_arr[3] = curr_arr[3] + \" \" + token.text\n",
    "                    curr_arr[3] = curr_arr[3].strip()\n",
    "                index+=1\n",
    "        else:\n",
    "            i = i.replace(\"to taste\", \"\")\n",
    "            doc = nlp(i)\n",
    "            curr_arr[0] = \"to taste\"\n",
    "            for token in doc:\n",
    "                if token.dep_ == \"ROOT\":\n",
    "                    curr_arr[2] = token.text\n",
    "                else:\n",
    "                    curr_arr[3] = curr_arr[3] + \" \" + token.text\n",
    "                    curr_arr[3] = curr_arr[3].strip()\n",
    "        ingredients_parsed.append(curr_arr)\n",
    "        #print(token.text, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop, token.children, token.head)\n",
    "        #displacy.render(doc, style=\"dep\") # change to serve when we go to python\n",
    "\n",
    "\n",
    "    \n",
    "    return ingredients_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "signal-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods Function\n",
    "\n",
    "def find_methods(step):\n",
    "    newstep = step\n",
    "\n",
    "    doc = nlp(newstep)\n",
    "    methods = {}\n",
    "    timescale = {}\n",
    "    times = {}\n",
    "    connector = {}\n",
    "    for i, entity in enumerate(doc):\n",
    "        if entity.pos_ == \"VERB\":  #and str(entity.head) == str(entity.text)\n",
    "            methods[entity] = {'times':[], 'scale':None}\n",
    "        if entity.dep_ == \"pobj\":\n",
    "            timescale[entity] = entity.head\n",
    "        if entity.pos_ == \"NUM\" and (entity.dep_ == 'quantmod' or entity.dep_ == 'nummod'):\n",
    "            times[entity] = entity.head\n",
    "#         for m in methods:\n",
    "#             if entity.head == m and str(entity.text) != str(entity.head):\n",
    "#                 connector[entity] = m\n",
    "        if str(entity.text) != str(entity.head):\n",
    "            connector[entity] = entity.head\n",
    "\n",
    "    for m in methods:\n",
    "        if m in connector:\n",
    "            n = connector[m]\n",
    "            if n not in methods:\n",
    "                connector[n] = m\n",
    "                connector.pop(m)\n",
    "    \n",
    "    for scale in timescale:\n",
    "        while timescale[scale] not in methods:\n",
    "            if timescale[scale] not in connector:\n",
    "                print(\"Error in dependency parsing\")\n",
    "                break\n",
    "            timescale[scale] = connector[timescale[scale]]\n",
    "            \n",
    "    for time in times:\n",
    "        if times[time] in times:\n",
    "            times[time] = times[times[time]]\n",
    "    \n",
    "    for s in timescale:\n",
    "        for t in times:\n",
    "            if times[t] == s:\n",
    "                methods[timescale[s]]['times'].append(float(str(t)))\n",
    "        methods[timescale[s]]['scale'] = str(s)\n",
    "        \n",
    "    M = {}\n",
    "    for m in methods:\n",
    "        if methods[m]['scale'] == 'seconds':\n",
    "            methods[m]['times'] = [t/60 for t in methods[m]['times']]\n",
    "            methods[m]['scale'] = 'minutes'\n",
    "        elif methods[m]['scale'] == 'hours':\n",
    "            methods[m]['times'] = [t*60 for t in methods[m]['times']]\n",
    "            methods[m]['scale'] = 'minutes'\n",
    "            \n",
    "        val = methods[m]\n",
    "        M[str(m).lower()] = val\n",
    "    \n",
    "    ## get indices of times, pobj (minutes, hours, days, etc), and method, and use distance to see which ones belong to which one.\n",
    "\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "comic-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools function\n",
    "\n",
    "def find_tool(sentence,filter_list):\n",
    "    in_list = ['in','into','on','to']\n",
    "    tools =[]\n",
    "    not_tools = []\n",
    "    sent = nlp(sentence.lower())\n",
    "    for chunk in sent.noun_chunks:\n",
    "#         and chunk.root.head.pos_ != 'VERB' and not chunk.root.is_sent_start\n",
    "        if chunk.root.text not in filter_list and chunk.root.head.text in in_list:\n",
    "            tools.append(chunk.root.text)\n",
    "#     for token in sent:\n",
    "#         if token.text not in too and token.pos_ == 'NOUN':\n",
    "#             not_tools.append(token.text)\n",
    "    return tools, not_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "celtic-crack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps function\n",
    "\n",
    "def write_step(ingredients, tools, methods):\n",
    "    lingredients = len(ingredients)\n",
    "    lmethods = len(methods)\n",
    "    ltools = len(tools)\n",
    "    \n",
    "    if \n",
    "    \n",
    "    if lmethods != ltools:\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    # get ingredients first\n",
    "    foods = [i for i in ingredients if i in sentence]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
